# Recommendation Systems: Ending CS BA Project
## Theorical Project in the PDF file:
Latex project made on Overleaf, with it's pdf and images available in this repository:
**Overleaf**: https://overleaf.com
**PDF**: On the root folder or in
**ArXiv**:
**Images**: On the "Images" folder
## Practical Project:
### Installing the Venv:
I called venv, and the sequence is in the requirements.txt, suposing the python venv is already installed in your device you will:
Create the venv called "firstEnv":
**python3 -m venv firstEnv**
Activate it:
**source firstEnv/bin/activate**
Install jupyter notebook that we'll need to make the work presentable.
**pip3 install jupyter**
TensorFlow version 1, for all the machine learning stuff
**pip3 install tensorflow**
Keras to make AI models even easier to implement
**pip3 install keras**
MatPlotLib for our data visualization
**pip3 install matplotlib**
TQDM for ProgressBars in out jupyter notebook or terminal
**pip3 install tqdm**
Scikit-Learn for it's extremely efficient implementations of a few famous algorithms
**pip3 install scikit-learn**
Pandas for all our dataframes and CSV manipulation
**python3 -m pip install --upgrade pandas**
Surprise for some already done recommender algorithms to compare
**pip3 install surprise**
Numpy and Scipy for dealing with large arrays
**pip3 install numpy**
**pip3 install scipy**
### Versions that I used:
Python: 3.7.3<br/>
TensorFlow: 1.14.0<br/>
Keras: 2.2.4<br/>
MatplotLib: 3.1.1<br/>
TQDM: 4.36.1<br/>
Scikit-Learn: 0.21.3<br/>
Surprise: 0.1<br/>
Scipy: 1.3.0<br/>
Pandas: 0.25.1<br/>
Jupyter Notebook: 1.0.0<br/>
Markdown: 3.1.1<br/>
Numpy: 1.16.4<br/>
Pip: 19.3<br/>
### Folders Structure Explained:
 - ML_Dataset Folder: Contains the Movie Lens Smallest and 27M datasets, it's on .gitignore because all files beyond the original dataset is generated by executing the scripts on this project.
 - tutorials_tf Folder: TensorFlow tutorial that I did just to learn a bit before the project itself, nothing related
 - first_env Folder: Also on .gitignore, it's basically the python virtual enviroment that we are going to install all we need. I caled first because, maybe, in the future, gonna have another ones to try different combinations of libraries like TensorFlow 2.0, Seaborn instead of Malplotlib. But this still not happened
 - main Folder: Literally the main, all the python scripts are here. Divided in the following subfolders:
 -- dataNormalization Folder: Script runned first, to normalize the data and make easier to run the next scripts, you'll run in the following order:
    * rating_norm.py will turn the ratings from range 0.5-5.0 to 0.1-1, in a new file ratings_norm.csv.
    * crop_tags.py will crop the tags that have very little correlation to the movie, if the relevance is lower than 0.3 it'll be cropped off.
    * join_tag_tagid.py will join the column generated by crop_tags with the genome_tags.csv and make a new csv genome_labels.csv that unites all the tags data that matters.
 -- dataPrep folder: The data preparation scripts, this includes the loading of datasets, the long tail crop, the dimensionality reduction and clusterization.
    * longTail_crop.py: It will crop the movies that are too little rated, and the user that evaluated too little movies, to make our data less noisy.
    * user_profile.py: Creates the movie_profile.csv and user_profile.csv the caractheristic vector that define them, in this recommender systems.
    * dim_reductor.py: Will reduce dimensionality in this profiles rewriting this same csv's.
    * movies_cluster.py: Responsible for running the cluster algorithm over the movies dataset, and creating the file: movies_cluster.csv
    * user_cluster.py: Responsible for running the cluster algorithm over the movies dataset, and creating the file: users_cluster.csv
    * load_dataset.py: This one will just load the original csv's into more strategic ones, a training and a test set called training_movies.csv and test_movies.csv